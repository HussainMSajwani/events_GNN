{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b5429f-14bf-408b-b53f-bf56aad9e674",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#torch.set_default_dtype(torch.float32) \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mASLDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mletters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_processing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_val_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:63\u001b[0m, in \u001b[0;36mASLDataset.__init__\u001b[0;34m(self, root, letters, beta, n_samples, radius, max_n_neighbors, overwrite_processing, train_val_test, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     61\u001b[0m root \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m train_val_test\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#print(self.raw_file_names)\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_filter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch_geometric/data/dataset.py:87\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch_geometric/data/dataset.py:164\u001b[0m, in \u001b[0;36mDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m osp\u001b[38;5;241m.\u001b[39mexists(f) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(f) \u001b[38;5;241m!=\u001b[39m _repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_filter):\n\u001b[1;32m    158\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `pre_filter` argument differs from the one used in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe pre-processed version of this dataset. If you want to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake use of another pre-fitering technique, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelete \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{self.processed_dir}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files_exist(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_paths\u001b[49m):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch_geometric/data/dataset.py:137\u001b[0m, in \u001b[0;36mDataset.processed_paths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocessed_paths\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The absolute filepaths that must be present in order to skip\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    processing.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     files \u001b[38;5;241m=\u001b[39m to_list(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessed_file_names\u001b[49m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:90\u001b[0m, in \u001b[0;36mASLDataset.processed_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir \u001b[38;5;241m/\u001b[39m letter \n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m#all_files += [prefix / f\"{letter}_{str(sample_id).zfill(4)}.pt\" for sample_id in range(1, 4201)]\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     all_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [glob(\u001b[38;5;28mstr\u001b[39m(prefix \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m)] \n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_files\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:158\u001b[0m, in \u001b[0;36mASLDataset.len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_file_names\u001b[49m)\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:81\u001b[0m, in \u001b[0;36mASLDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir \u001b[38;5;241m/\u001b[39m letter \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#all_files += [prefix / f\"{letter}_{str(sample_id).zfill(4)}.mat\" for sample_id in range(1, 4201)]\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     all_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [glob(\u001b[38;5;28mstr\u001b[39m(prefix \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m)]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_files\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:158\u001b[0m, in \u001b[0;36mASLDataset.len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_file_names\u001b[49m)\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:81\u001b[0m, in \u001b[0;36mASLDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir \u001b[38;5;241m/\u001b[39m letter \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#all_files += [prefix / f\"{letter}_{str(sample_id).zfill(4)}.mat\" for sample_id in range(1, 4201)]\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     all_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [glob(\u001b[38;5;28mstr\u001b[39m(prefix \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m)]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_files\n",
      "    \u001b[0;31m[... skipping similar frames: ASLDataset.len at line 158 (1478 times), ASLDataset.raw_file_names at line 81 (1477 times)]\u001b[0m\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:81\u001b[0m, in \u001b[0;36mASLDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir \u001b[38;5;241m/\u001b[39m letter \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#all_files += [prefix / f\"{letter}_{str(sample_id).zfill(4)}.mat\" for sample_id in range(1, 4201)]\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     all_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [glob(\u001b[38;5;28mstr\u001b[39m(prefix \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m)]\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_files\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:158\u001b[0m, in \u001b[0;36mASLDataset.len\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_file_names\u001b[49m)\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:79\u001b[0m, in \u001b[0;36mASLDataset.raw_file_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m all_files \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m letter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletters:\n\u001b[0;32m---> 79\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_dir\u001b[49m \u001b[38;5;241m/\u001b[39m letter \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m#all_files += [prefix / f\"{letter}_{str(sample_id).zfill(4)}.mat\" for sample_id in range(1, 4201)]\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     all_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [glob(\u001b[38;5;28mstr\u001b[39m(prefix \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mletter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen)]\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/ASLDataset.py:67\u001b[0m, in \u001b[0;36mASLDataset.raw_dir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraw_dir\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/pathlib.py:935\u001b[0m, in \u001b[0;36mPurePath.__truediv__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__truediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/pathlib.py:713\u001b[0m, in \u001b[0;36mPurePath._make_child\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_child\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m--> 713\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mjoin_parsed_parts(\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parts, drv, root, parts)\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_parsed_parts(drv, root, parts)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/pathlib.py:676\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument should be a str object or an os.PathLike \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject returning str, not \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    675\u001b[0m                 \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(a))\n\u001b[0;32m--> 676\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flavour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/pathlib.py:69\u001b[0m, in \u001b[0;36m_Flavour.parse_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m     67\u001b[0m altsep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maltsep\n\u001b[1;32m     68\u001b[0m drv \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 69\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m part:\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "from imports.ASLDataset import ASLDataset\n",
    "import torch\n",
    "#torch.set_default_dtype(torch.float32) \n",
    "\n",
    "l = ASLDataset(letters=['a', 'b'], overwrite_processing=True, train_val_test='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c1e5b-d105-4d68-a7af-920660439108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ed4af-3e09-4922-818c-520ab1c8fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "data_train, data_test = torch.utils.data.random_split(\n",
    "    dataset, \n",
    "    [int(n*0.8), int(n*0.2)], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8ab6caf7-58d2-40fc-85f5-1063df941d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "59e3ad3e-dde3-4636-8f7f-b523fa08b4cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [128]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51edf580-ab38-4b16-ab04-f5413cf5c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c43a482ce74743b5104ed398836be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a_train:   0%|          | 0/3360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf18e2d79d548fca9980a05c99be0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a_val:   0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b194ef8b6ad5403db2ce0a87c468cb17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "a_test:   0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def random_split(x, prop=[0.80, 0.1, 0.1]):\n",
    "    assert sum(prop) == 1, \"sum of prop must be 1\"\n",
    "    n = len(x)\n",
    "    np.random.shuffle(x)\n",
    "    \n",
    "    train_idx = int(prop[0]*n) \n",
    "    val_idx = train_idx + int(prop[1]*n) \n",
    "    out = np.split(x, [train_idx, val_idx])\n",
    "    return {'train': out[0], 'val': out[1], 'test': out[2]}\n",
    "    \n",
    "zpad = lambda x, n: str(x).zfill(n)\n",
    "\n",
    "np.random.seed(0)\n",
    "letters = ['a']\n",
    "datadir = Path(\"/home/hussain/data/event_based_sign_lang/\")\n",
    "new_dir = datadir.parent / 'event_based_sign_lang'\n",
    "for letter in letters:\n",
    "    split = random_split(np.arange(1, 4201))\n",
    "    for key, part in split.items():\n",
    "        for i, idx in enumerate(tqdm(part, desc=f'{letter}_{key}')):\n",
    "            raw = datadir / 'raw' / letter / f'{letter}_{str(idx).zfill(4)}.mat'\n",
    "            proc = datadir / 'processed' / letter / f'{letter}_{str(idx).zfill(4)}.pt'\n",
    "            \n",
    "            shutil.move(raw, new_dir / key / 'raw' / letter / f'{letter}_{zpad(i, 4)}.mat')\n",
    "            shutil.move(proc, new_dir / key / 'processed' / letter / f'{letter}_{zpad(i, 4)}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5bae3f80-fd93-4452-adef-dd11f7ba0b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(f\"/home/hussain/data/event_based_sign_lang/{letters}/a/a_0001.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4bd5056f-6016-4ceb-8fe5-5a7939076606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4200"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3360 + 420*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "430ecac9-c136-472e-9ad1-2da1b9cef639",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m train, val, test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4201\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val, test = train_test_split(range(1, 4201))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f02e6bbf-0e9a-4a2d-a806-7dd23a131598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2730], pos=[2730, 3], num_nodes=2730, edge_index=[2, 61551], edge_attr=[61551, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imports.sample_io import load_sample\n",
    "from imports.utils import make_graph\n",
    "\n",
    "a = load_sample('a', sample_id=222)\n",
    "g = make_graph(a)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21457806-23e5-4fec-8bd6-3eb71dae2017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_vs_b.ipynb\t imports\t\t __pycache__\t   run.py\n",
      "data\t\t log\t\t\t requirements.txt  slurm.sh\n",
      "environment.yml  network_analysis.ipynb  results\t   testing\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4f04351-0a69-4ec2-a229-e0aa42bee904",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [3049, 6887, 3106, 3924, 5302, 3480, 2584, 1873, 4922, 6800, 5240, 564, 2903, 6039, 1464, 2107, 5998, 4429, 1199, 7953, 2273, 6345, 2617, 346, 2888, 5452, 8320, 6891, 20, 2813, 530, 1478, 2922, 6402, 4854, 4692, 1708, 5137, 5189, 551, 6660, 6133, 1614, 182, 5770, 4622, 75, 6299, 889, 6857, 4157, 669, 1647, 2049, 4852, 1102, 1600, 2910, 4260, 1584, 5366, 5362, 2261, 8085, 5731, 3481, 5288, 7338, 3085, 1696, 6161, 7763, 6629, 975, 1966, 8004, 3471, 3730, 3222, 7281, 3602, 7180, 7249, 4499, 7103, 2238, 6696, 6275, 3877, 2062, 7971, 6495, 4179, 4770, 3512, 6011, 5503, 5194, 5345, 3186, 6414, 2090, 4935, 3663, 803, 8177, 478, 7327, 3094, 7384, 6922, 6195, 148, 8371, 4695, 6031, 767, 8337, 6180, 728, 2006, 3533, 1590, 4094, 8083, 8135, 3321, 1033, 3945, 6132, 6928, 1436, 1080, 3415, 7265, 966, 7724, 969, 1880, 7094, 860, 5468, 515, 3013, 251, 1860, 7714, 942, 8305, 8234, 5344, 3497, 2896, 558, 1117, 8126, 7439, 2624, 2499, 972, 8390, 750, 3, 3757, 887, 1811, 7784, 1904, 7811, 1625, 289, 4044, 4009, 1959, 1025, 3182, 1249, 2136, 4817, 4305, 4941, 1194, 6913, 7584, 1492, 4522, 4163, 6967, 6904, 6855, 7500, 1828, 8228, 5507, 1429, 7092, 4147, 3142, 7164, 397, 2859, 1694, 1728, 3465, 1755, 2638, 5083, 1045, 5097, 3034, 1699, 3725, 2051, 7922, 6589, 6611, 3388, 533, 2298, 6806, 2962, 2189, 5340, 6424, 4878, 3089, 6466, 992, 4015, 1713, 5863, 1263, 4287, 5709, 340, 1344, 4901, 7642, 572, 7371, 861, 7000, 2339, 1313, 4185, 7358, 1623, 3608, 218, 5748, 2736, 674, 4640, 2458, 2997, 5115, 6403, 5433, 2073, 6996, 4805, 4159, 6297, 4166, 2418, 5579, 2727, 5967, 2215, 946, 4136, 7108, 7036, 3737, 6295, 8334, 1526, 3408, 6907, 6843, 4124, 5134, 5055, 5784, 6715, 7643, 7395, 4943, 2580, 2616, 6337, 114, 6134, 1939, 7995, 2091, 806, 2547, 3614, 5019, 5203, 8288, 2214, 3275, 1620, 2806, 7376, 6241, 8123, 7942, 4690, 6395, 4336, 6983, 3721, 6250, 1751, 3547, 7881, 4508, 6819, 372, 5310, 4745, 5227, 3470, 4535, 7966, 762, 2231, 763, 7370, 4977, 3117, 6964, 8221, 694, 3859, 1356, 7234, 2067, 1806, 7843, 8312, 962, 5074, 7599, 6511, 8333, 1982, 2971, 2286, 3910, 1709, 6995, 2301, 5650, 5873, 4524, 2218, 5589, 7021, 8081, 5876, 4644, 5725, 974, 2324, 7278, 1646, 3261, 4898, 2634, 4757, 22, 1825, 3197, 6539, 568, 6574, 5457, 1394, 5436, 5101, 1520, 677, 5733, 6557, 773, 7446, 7800, 5518, 4191, 3709, 7341, 5405, 2945, 3157, 3110, 691, 6038, 4297, 8192, 4842, 6197, 1026, 7221, 6191, 953, 8235, 4518, 6316, 2123, 6004, 5525, 4701, 1189, 6422, 5411, 1543, 8066, 2391, 173, 2363, 4171, 5114, 8104, 3744, 3876, 4950, 5237, 7837, 5081, 3054, 6346, 2087, 2745, 6591, 2546, 1975, 7755, 4478, 5195, 1264, 1425, 3184, 7728, 7519, 6597, 1761, 7884, 5122, 2244, 310, 787, 3983, 6260, 2296, 238, 2486, 1164, 808, 552, 5230, 8034, 8105, 231, 4417, 4421, 5304, 3994, 3304, 4214, 4894, 2721, 7211, 3414, 1248, 1519, 5553, 3954, 2185, 2881, 3684, 3345, 4763, 8329, 6760, 2647, 6710, 6230, 6072, 2316, 4334, 7156, 2722, 224, 1913, 6681, 8106, 7751, 8087, 3871, 6610, 4804, 5485, 1131, 2994, 7962, 1016, 2493, 2656, 1032, 7540, 2728, 1322, 5771, 1474, 4754, 494, 1457, 7682, 2907, 4762, 3968, 5239, 5341, 3819, 3291, 4645, 209, 5819, 2130, 3203, 8028, 3883, 7305, 7680, 5167, 5372, 6795, 807, 3485, 5837, 3324, 3987, 1012, 7264, 6971, 7241, 520, 5141, 118, 5530, 8188, 1764, 2824, 8074, 5021, 6669, 2715, 1010, 3147, 7992, 1522, 7677, 3962, 2639, 2431, 5406, 5927, 1914, 2987, 4399, 3548, 5427, 5823, 7037, 6571, 6054, 1850, 4959, 6664, 2475, 2227, 4960, 6818, 2216, 146, 2689, 5202, 3574, 5073, 5275, 3206, 4463, 7367, 3088, 8082, 3048, 2427, 3099, 2471, 5197, 3466, 7597, 1134, 766, 6573, 5955, 178, 5076, 986, 5647, 7227, 5668, 3544, 4698, 1450, 5401, 411, 3041, 321, 6434, 5611, 8316, 7125, 1316, 3451, 1273, 6630, 5907, 1341, 7835, 5325, 3816, 2355, 8303, 5674, 8068, 7152, 3352, 4850, 3330, 6029, 1226, 5383, 670, 3557, 7432, 1212, 1407, 7910, 5078, 8009, 7505, 2953, 464, 7239, 4507, 841, 1663, 4670, 3761, 599, 7936, 4732, 5455, 4261, 7423, 1789, 4806, 565, 2726, 7359, 6044, 8262, 4746, 7303, 4855, 2913, 2671, 3459, 1768, 4080, 1517, 8343, 5763, 6752, 3580, 4065, 3031, 1604, 7742, 2074, 2995, 3234, 7631, 5161, 7437, 7287, 5922, 1943, 2253, 2266, 7210, 2540, 7577, 3072, 5691, 3255, 160, 7482, 7059, 4031, 259, 542, 6328, 4982, 3853, 1533, 4315, 2930, 7067, 4575, 8397, 4378, 2618, 1068, 510, 3916, 7805, 4318, 5154, 3438, 977, 8271, 1210, 4224, 2852, 3201, 472, 5702, 5852, 6503, 5815, 4218, 6836, 441, 3650, 7284, 5481, 5279, 1855, 7485, 1793, 832, 2205, 4234, 6502, 537, 4587, 2654, 3439, 7569, 8002, 4740, 5333, 4052, 4388, 7055, 6410, 6109, 5919, 4739, 414, 3340, 4840, 8368, 4477, 6192, 5459, 5632, 1929, 6731, 3163, 2857, 8001, 3995, 169, 5100, 7679, 2151, 7464, 7935, 5317, 4905, 2785, 7110, 2128, 6118, 4643, 4230, 830, 6820, 6184, 869, 7004, 4212, 5196, 6112, 2289, 6366, 3784, 6358, 5734, 3381, 93, 6081, 7841, 5803, 3153, 3432, 7725, 1949, 6238, 7828, 2108, 633, 8258, 4295, 3656, 5754, 3242, 2406, 3549, 571, 6734, 4702, 3285, 6634, 6430, 4720, 1732, 2422, 7315, 1648, 1627, 1652, 5257, 3204, 7262, 753, 1389, 7257, 4896, 7417, 7877, 344, 3192, 5379, 3446, 1542, 316, 3573, 6190, 225, 6602, 7379, 6274, 7427, 7009, 4245, 6642, 6768, 6662, 7917, 7836, 7237, 7419, 2739, 1790, 3960, 7990, 33, 3595, 1148, 6355, 4759, 4150, 8344, 1444, 827, 6898, 608, 7226, 1136, 1231, 6087, 4096, 8287, 5569, 8246, 7559, 1094, 2609, 5667, 3862, 3413, 6565, 2305, 1127, 2284, 6485, 6024, 6546, 2378, 6123, 8291, 4904, 1958, 1129, 1236, 6101, 7770, 213, 1552, 7980, 7509, 7997, 2604, 8256, 2054, 5604, 2762, 369, 3467, 5617, 8011, 3584, 6838, 5966, 6618, 286, 5884, 2300, 5854, 6621, 5251, 6884, 4231, 3159, 5677, 4856, 7039, 1595, 6108, 7850, 1480, 7788, 4131, 4008, 2312, 4001, 3456, 2254, 6169, 926, 4331, 8031, 8131, 5628, 3788, 4566, 501, 4976, 761, 32, 8174, 2666, 2402, 6858, 3959, 1863, 4262, 7629, 4837, 3937, 31, 5282, 302, 3931, 285, 41, 1208, 5059, 1166, 7952, 7560, 1902, 8159, 3621, 4722, 4581, 3511, 795, 4501, 1572, 7296, 6243, 4921, 7711, 4387, 3504, 7295, 1716, 3500, 5540, 7889, 2364, 7978, 4035, 3407, 5680, 1445, 6335, 4607, 3272, 3835, 5376, 4801, 3047, 3339, 2650, 1495, 8171, 6512, 4682, 7068, 3745, 6147, 2777, 1037, 7960, 7916, 4814, 2690, 4310, 3478, 5567, 5956, 7455, 5562, 7717, 208, 7006, 6921, 7167, 2120, 7698, 7012, 4207, 7430, 5838, 3988, 4003, 915, 5243, 2376, 4436, 4973, 4676, 5351, 8097, 699, 1822, 5653, 3605, 2667, 6878, 836, 2573, 3810, 5663, 1591, 3935, 477, 4046, 693, 3703, 293, 5749, 4393, 1501, 309, 7888, 652, 5058, 287, 5736, 1800, 8187, 3689, 3824, 5746, 4591, 8194, 1779, 2291, 1083, 957, 1839, 5408, 8244, 5874, 495, 7635, 4627, 7674, 6389, 5853, 5679, 5517, 5117, 1427, 3037, 2918, 7771, 3524, 4626, 6139, 1363, 2131, 6105, 5834, 2800, 337, 5659, 8166, 5200, 7261, 627, 7254, 7172, 5226, 92, 6876, 2879, 8374, 176, 7040, 4957, 7269, 3629, 7596, 3746, 6528, 2441, 3990, 5545, 5041, 930, 4738, 6315, 7204, 8307, 2225, 1490, 5550, 3042, 1292, 339, 1541, 1011, 2714, 602, 6013, 404, 4380, 3280, 4781, 1674, 1848, 3193, 1295, 1354, 4074, 698, 5781, 868, 80, 5088, 2776, 4120, 539, 7866, 1844, 935, 23, 575, 1891, 7157, 6849, 6198, 2442, 8090, 759, 2725, 5184, 5421, 1774, 4917, 3091, 2664, 4771, 5397, 2691, 6055, 2293, 823, 3015, 7339, 6370, 5685, 7143, 4798, 2880, 1162, 5949, 2564, 5508, 1976, 8062, 7267, 1396, 4314, 4151, 7175, 6686, 1757, 256, 3364, 7945, 4831, 4010, 5319, 1698, 4910, 7387, 72, 5986, 216, 3226, 4951, 6684, 4386, 56, 1645, 3850, 5171, 2641, 7421, 5132, 4784, 2612, 2229, 802, 2730, 2070, 3566, 3248, 6067, 6301, 4435, 2008, 4232, 7064, 1169, 6947, 765, 7169, 5213, 3095, 1838, 5070, 1952, 6089, 1392, 4317, 192, 3694, 2141, 6255, 1448, 7601, 6645, 2700, 5645, 7138, 7715, 2278, 3832, 3634, 6266, 4500, 6832, 7766, 6777, 4146, 2415, 4240, 4221, 7733, 7566, 7613, 4934, 6853, 1485, 2199, 5217, 8226, 2874, 7512, 7831, 4703, 2033, 5844, 3144, 2057, 7116, 2177, 150, 2179, 5204, 2076, 2640, 2494, 5566, 991, 306, 6361, 6667, 2125, 4178, 6725, 2524, 850, 2840, 7535, 1220, 7294, 952, 2037, 8248, 6341, 6778, 3768, 4747, 4758, 2876, 457, 3838, 2743, 7849, 4999, 1214, 7532, 5735, 5571, 1072, 2709, 3140, 779, 7778, 3129, 5936, 6976, 7361, 4526, 5394, 5729, 7658, 2428, 3443, 4126, 5474, 734, 6314, 5858, 8036, 1953, 5516, 2754, 4183, 7819, 5353, 498, 86, 6877, 4860, 4186, 1613, 3922, 3997, 7390, 6615, 6614, 3194, 6692, 7176, 3173, 629, 3368, 2050, 2154, 1615, 5003, 2955, 5942, 8071, 876, 8006, 1554, 4423, 7651, 4674, 7302, 5280, 5607, 4753, 8088, 7982, 1227, 2533, 4091, 2864, 2525, 427, 7489, 1925, 8379, 2371, 5045, 2345, 1217, 1282, 2899, 838, 3063, 7862, 307, 512, 5527, 6900, 1679, 7927, 8150, 2319, 610, 1955, 4641, 6639, 4748, 2465, 1230, 3719, 1715, 1380, 1228, 2140, 6620, 2946, 319, 7905, 4827, 6185, 2251, 1511, 3847, 2235, 5168, 2466, 188, 7533, 2322, 5219, 7248, 4345, 7429, 1758, 7034, 1351, 1672, 6352, 5028, 6130, 85, 6334, 3882, 3825, 4455, 3299, 5390, 951, 1536, 6086, 7511, 2644, 1664, 5361, 6745, 6063, 4404, 6592, 7277, 2592, 6239, 3776, 2648, 4034, 5712, 4558, 1632, 5346, 7406, 7418, 1184, 749, 4284, 5994, 6600, 4116, 5036, 3115, 7695, 1798, 3929, 8284, 7744, 4254, 5892, 3264, 5818, 8046, 531, 4793, 1857, 6215, 5833, 3578, 2670, 4079, 7897, 3593, 4691, 904, 1369, 8339, 2758, 5909, 583, 2166, 4932, 4275, 8151, 5000, 3540, 1144, 872, 5245, 7752, 2608, 973, 8115, 1951, 4446, 7579, 5013, 1500, 7091, 5872, 6014, 5543, 5449, 144, 706, 4012, 6647, 2948, 6077, 3236, 152, 97, 1776, 7968, 90, 2041, 4502, 1887, 1229, 7514, 4972, 4223, 4123, 2157, 3926, 1463, 2587, 1938, 6333, 6599, 4830, 8245, 1243, 6283, 7173, 7785, 943, 2702, 2933, 2032, 4177, 4538, 5943, 2082, 1049, 2815, 5812, 6496, 4868, 2449, 6977, 3421, 5697, 4648, 2887, 2202, 116, 522, 1079, 4985, 1297, 3113, 4603, 1137, 2208, 5029, 3138, 6972, 4853, 4411, 4114, 2109, 956, 7286, 7567, 1753, 6478, 4704, 6464, 3726, 1245, 8300, 8195, 6300, 7057, 815, 3673, 2021, 1234, 768, 5062, 3923, 5082, 6814, 345, 3554, 665, 1053, 4956]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff4ad067-a76d-4e0a-919e-160dfeb5114a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3049,\n",
       " 6887,\n",
       " 3106,\n",
       " 3924,\n",
       " 5302,\n",
       " 3480,\n",
       " 2584,\n",
       " 1873,\n",
       " 4922,\n",
       " 6800,\n",
       " 5240,\n",
       " 564,\n",
       " 2903,\n",
       " 6039,\n",
       " 1464,\n",
       " 2107,\n",
       " 5998,\n",
       " 4429,\n",
       " 1199,\n",
       " 7953,\n",
       " 2273,\n",
       " 6345,\n",
       " 2617,\n",
       " 346,\n",
       " 2888,\n",
       " 5452,\n",
       " 8320,\n",
       " 6891,\n",
       " 20,\n",
       " 2813,\n",
       " 530,\n",
       " 1478,\n",
       " 2922,\n",
       " 6402,\n",
       " 4854,\n",
       " 4692,\n",
       " 1708,\n",
       " 5137,\n",
       " 5189,\n",
       " 551,\n",
       " 6660,\n",
       " 6133,\n",
       " 1614,\n",
       " 182,\n",
       " 5770,\n",
       " 4622,\n",
       " 75,\n",
       " 6299,\n",
       " 889,\n",
       " 6857,\n",
       " 4157,\n",
       " 669,\n",
       " 1647,\n",
       " 2049,\n",
       " 4852,\n",
       " 1102,\n",
       " 1600,\n",
       " 2910,\n",
       " 4260,\n",
       " 1584,\n",
       " 5366,\n",
       " 5362,\n",
       " 2261,\n",
       " 8085,\n",
       " 5731,\n",
       " 3481,\n",
       " 5288,\n",
       " 7338,\n",
       " 3085,\n",
       " 1696,\n",
       " 6161,\n",
       " 7763,\n",
       " 6629,\n",
       " 975,\n",
       " 1966,\n",
       " 8004,\n",
       " 3471,\n",
       " 3730,\n",
       " 3222,\n",
       " 7281,\n",
       " 3602,\n",
       " 7180,\n",
       " 7249,\n",
       " 4499,\n",
       " 7103,\n",
       " 2238,\n",
       " 6696,\n",
       " 6275,\n",
       " 3877,\n",
       " 2062,\n",
       " 7971,\n",
       " 6495,\n",
       " 4179,\n",
       " 4770,\n",
       " 3512,\n",
       " 6011,\n",
       " 5503,\n",
       " 5194,\n",
       " 5345,\n",
       " 3186,\n",
       " 6414,\n",
       " 2090,\n",
       " 4935,\n",
       " 3663,\n",
       " 803,\n",
       " 8177,\n",
       " 478,\n",
       " 7327,\n",
       " 3094,\n",
       " 7384,\n",
       " 6922,\n",
       " 6195,\n",
       " 148,\n",
       " 8371,\n",
       " 4695,\n",
       " 6031,\n",
       " 767,\n",
       " 8337,\n",
       " 6180,\n",
       " 728,\n",
       " 2006,\n",
       " 3533,\n",
       " 1590,\n",
       " 4094,\n",
       " 8083,\n",
       " 8135,\n",
       " 3321,\n",
       " 1033,\n",
       " 3945,\n",
       " 6132,\n",
       " 6928,\n",
       " 1436,\n",
       " 1080,\n",
       " 3415,\n",
       " 7265,\n",
       " 966,\n",
       " 7724,\n",
       " 969,\n",
       " 1880,\n",
       " 7094,\n",
       " 860,\n",
       " 5468,\n",
       " 515,\n",
       " 3013,\n",
       " 251,\n",
       " 1860,\n",
       " 7714,\n",
       " 942,\n",
       " 8305,\n",
       " 8234,\n",
       " 5344,\n",
       " 3497,\n",
       " 2896,\n",
       " 558,\n",
       " 1117,\n",
       " 8126,\n",
       " 7439,\n",
       " 2624,\n",
       " 2499,\n",
       " 972,\n",
       " 8390,\n",
       " 750,\n",
       " 3,\n",
       " 3757,\n",
       " 887,\n",
       " 1811,\n",
       " 7784,\n",
       " 1904,\n",
       " 7811,\n",
       " 1625,\n",
       " 289,\n",
       " 4044,\n",
       " 4009,\n",
       " 1959,\n",
       " 1025,\n",
       " 3182,\n",
       " 1249,\n",
       " 2136,\n",
       " 4817,\n",
       " 4305,\n",
       " 4941,\n",
       " 1194,\n",
       " 6913,\n",
       " 7584,\n",
       " 1492,\n",
       " 4522,\n",
       " 4163,\n",
       " 6967,\n",
       " 6904,\n",
       " 6855,\n",
       " 7500,\n",
       " 1828,\n",
       " 8228,\n",
       " 5507,\n",
       " 1429,\n",
       " 7092,\n",
       " 4147,\n",
       " 3142,\n",
       " 7164,\n",
       " 397,\n",
       " 2859,\n",
       " 1694,\n",
       " 1728,\n",
       " 3465,\n",
       " 1755,\n",
       " 2638,\n",
       " 5083,\n",
       " 1045,\n",
       " 5097,\n",
       " 3034,\n",
       " 1699,\n",
       " 3725,\n",
       " 2051,\n",
       " 7922,\n",
       " 6589,\n",
       " 6611,\n",
       " 3388,\n",
       " 533,\n",
       " 2298,\n",
       " 6806,\n",
       " 2962,\n",
       " 2189,\n",
       " 5340,\n",
       " 6424,\n",
       " 4878,\n",
       " 3089,\n",
       " 6466,\n",
       " 992,\n",
       " 4015,\n",
       " 1713,\n",
       " 5863,\n",
       " 1263,\n",
       " 4287,\n",
       " 5709,\n",
       " 340,\n",
       " 1344,\n",
       " 4901,\n",
       " 7642,\n",
       " 572,\n",
       " 7371,\n",
       " 861,\n",
       " 7000,\n",
       " 2339,\n",
       " 1313,\n",
       " 4185,\n",
       " 7358,\n",
       " 1623,\n",
       " 3608,\n",
       " 218,\n",
       " 5748,\n",
       " 2736,\n",
       " 674,\n",
       " 4640,\n",
       " 2458,\n",
       " 2997,\n",
       " 5115,\n",
       " 6403,\n",
       " 5433,\n",
       " 2073,\n",
       " 6996,\n",
       " 4805,\n",
       " 4159,\n",
       " 6297,\n",
       " 4166,\n",
       " 2418,\n",
       " 5579,\n",
       " 2727,\n",
       " 5967,\n",
       " 2215,\n",
       " 946,\n",
       " 4136,\n",
       " 7108,\n",
       " 7036,\n",
       " 3737,\n",
       " 6295,\n",
       " 8334,\n",
       " 1526,\n",
       " 3408,\n",
       " 6907,\n",
       " 6843,\n",
       " 4124,\n",
       " 5134,\n",
       " 5055,\n",
       " 5784,\n",
       " 6715,\n",
       " 7643,\n",
       " 7395,\n",
       " 4943,\n",
       " 2580,\n",
       " 2616,\n",
       " 6337,\n",
       " 114,\n",
       " 6134,\n",
       " 1939,\n",
       " 7995,\n",
       " 2091,\n",
       " 806,\n",
       " 2547,\n",
       " 3614,\n",
       " 5019,\n",
       " 5203,\n",
       " 8288,\n",
       " 2214,\n",
       " 3275,\n",
       " 1620,\n",
       " 2806,\n",
       " 7376,\n",
       " 6241,\n",
       " 8123,\n",
       " 7942,\n",
       " 4690,\n",
       " 6395,\n",
       " 4336,\n",
       " 6983,\n",
       " 3721,\n",
       " 6250,\n",
       " 1751,\n",
       " 3547,\n",
       " 7881,\n",
       " 4508,\n",
       " 6819,\n",
       " 372,\n",
       " 5310,\n",
       " 4745,\n",
       " 5227,\n",
       " 3470,\n",
       " 4535,\n",
       " 7966,\n",
       " 762,\n",
       " 2231,\n",
       " 763,\n",
       " 7370,\n",
       " 4977,\n",
       " 3117,\n",
       " 6964,\n",
       " 8221,\n",
       " 694,\n",
       " 3859,\n",
       " 1356,\n",
       " 7234,\n",
       " 2067,\n",
       " 1806,\n",
       " 7843,\n",
       " 8312,\n",
       " 962,\n",
       " 5074,\n",
       " 7599,\n",
       " 6511,\n",
       " 8333,\n",
       " 1982,\n",
       " 2971,\n",
       " 2286,\n",
       " 3910,\n",
       " 1709,\n",
       " 6995,\n",
       " 2301,\n",
       " 5650,\n",
       " 5873,\n",
       " 4524,\n",
       " 2218,\n",
       " 5589,\n",
       " 7021,\n",
       " 8081,\n",
       " 5876,\n",
       " 4644,\n",
       " 5725,\n",
       " 974,\n",
       " 2324,\n",
       " 7278,\n",
       " 1646,\n",
       " 3261,\n",
       " 4898,\n",
       " 2634,\n",
       " 4757,\n",
       " 22,\n",
       " 1825,\n",
       " 3197,\n",
       " 6539,\n",
       " 568,\n",
       " 6574,\n",
       " 5457,\n",
       " 1394,\n",
       " 5436,\n",
       " 5101,\n",
       " 1520,\n",
       " 677,\n",
       " 5733,\n",
       " 6557,\n",
       " 773,\n",
       " 7446,\n",
       " 7800,\n",
       " 5518,\n",
       " 4191,\n",
       " 3709,\n",
       " 7341,\n",
       " 5405,\n",
       " 2945,\n",
       " 3157,\n",
       " 3110,\n",
       " 691,\n",
       " 6038,\n",
       " 4297,\n",
       " 8192,\n",
       " 4842,\n",
       " 6197,\n",
       " 1026,\n",
       " 7221,\n",
       " 6191,\n",
       " 953,\n",
       " 8235,\n",
       " 4518,\n",
       " 6316,\n",
       " 2123,\n",
       " 6004,\n",
       " 5525,\n",
       " 4701,\n",
       " 1189,\n",
       " 6422,\n",
       " 5411,\n",
       " 1543,\n",
       " 8066,\n",
       " 2391,\n",
       " 173,\n",
       " 2363,\n",
       " 4171,\n",
       " 5114,\n",
       " 8104,\n",
       " 3744,\n",
       " 3876,\n",
       " 4950,\n",
       " 5237,\n",
       " 7837,\n",
       " 5081,\n",
       " 3054,\n",
       " 6346,\n",
       " 2087,\n",
       " 2745,\n",
       " 6591,\n",
       " 2546,\n",
       " 1975,\n",
       " 7755,\n",
       " 4478,\n",
       " 5195,\n",
       " 1264,\n",
       " 1425,\n",
       " 3184,\n",
       " 7728,\n",
       " 7519,\n",
       " 6597,\n",
       " 1761,\n",
       " 7884,\n",
       " 5122,\n",
       " 2244,\n",
       " 310,\n",
       " 787,\n",
       " 3983,\n",
       " 6260,\n",
       " 2296,\n",
       " 238,\n",
       " 2486,\n",
       " 1164,\n",
       " 808,\n",
       " 552,\n",
       " 5230,\n",
       " 8034,\n",
       " 8105,\n",
       " 231,\n",
       " 4417,\n",
       " 4421,\n",
       " 5304,\n",
       " 3994,\n",
       " 3304,\n",
       " 4214,\n",
       " 4894,\n",
       " 2721,\n",
       " 7211,\n",
       " 3414,\n",
       " 1248,\n",
       " 1519,\n",
       " 5553,\n",
       " 3954,\n",
       " 2185,\n",
       " 2881,\n",
       " 3684,\n",
       " 3345,\n",
       " 4763,\n",
       " 8329,\n",
       " 6760,\n",
       " 2647,\n",
       " 6710,\n",
       " 6230,\n",
       " 6072,\n",
       " 2316,\n",
       " 4334,\n",
       " 7156,\n",
       " 2722,\n",
       " 224,\n",
       " 1913,\n",
       " 6681,\n",
       " 8106,\n",
       " 7751,\n",
       " 8087,\n",
       " 3871,\n",
       " 6610,\n",
       " 4804,\n",
       " 5485,\n",
       " 1131,\n",
       " 2994,\n",
       " 7962,\n",
       " 1016,\n",
       " 2493,\n",
       " 2656,\n",
       " 1032,\n",
       " 7540,\n",
       " 2728,\n",
       " 1322,\n",
       " 5771,\n",
       " 1474,\n",
       " 4754,\n",
       " 494,\n",
       " 1457,\n",
       " 7682,\n",
       " 2907,\n",
       " 4762,\n",
       " 3968,\n",
       " 5239,\n",
       " 5341,\n",
       " 3819,\n",
       " 3291,\n",
       " 4645,\n",
       " 209,\n",
       " 5819,\n",
       " 2130,\n",
       " 3203,\n",
       " 8028,\n",
       " 3883,\n",
       " 7305,\n",
       " 7680,\n",
       " 5167,\n",
       " 5372,\n",
       " 6795,\n",
       " 807,\n",
       " 3485,\n",
       " 5837,\n",
       " 3324,\n",
       " 3987,\n",
       " 1012,\n",
       " 7264,\n",
       " 6971,\n",
       " 7241,\n",
       " 520,\n",
       " 5141,\n",
       " 118,\n",
       " 5530,\n",
       " 8188,\n",
       " 1764,\n",
       " 2824,\n",
       " 8074,\n",
       " 5021,\n",
       " 6669,\n",
       " 2715,\n",
       " 1010,\n",
       " 3147,\n",
       " 7992,\n",
       " 1522,\n",
       " 7677,\n",
       " 3962,\n",
       " 2639,\n",
       " 2431,\n",
       " 5406,\n",
       " 5927,\n",
       " 1914,\n",
       " 2987,\n",
       " 4399,\n",
       " 3548,\n",
       " 5427,\n",
       " 5823,\n",
       " 7037,\n",
       " 6571,\n",
       " 6054,\n",
       " 1850,\n",
       " 4959,\n",
       " 6664,\n",
       " 2475,\n",
       " 2227,\n",
       " 4960,\n",
       " 6818,\n",
       " 2216,\n",
       " 146,\n",
       " 2689,\n",
       " 5202,\n",
       " 3574,\n",
       " 5073,\n",
       " 5275,\n",
       " 3206,\n",
       " 4463,\n",
       " 7367,\n",
       " 3088,\n",
       " 8082,\n",
       " 3048,\n",
       " 2427,\n",
       " 3099,\n",
       " 2471,\n",
       " 5197,\n",
       " 3466,\n",
       " 7597,\n",
       " 1134,\n",
       " 766,\n",
       " 6573,\n",
       " 5955,\n",
       " 178,\n",
       " 5076,\n",
       " 986,\n",
       " 5647,\n",
       " 7227,\n",
       " 5668,\n",
       " 3544,\n",
       " 4698,\n",
       " 1450,\n",
       " 5401,\n",
       " 411,\n",
       " 3041,\n",
       " 321,\n",
       " 6434,\n",
       " 5611,\n",
       " 8316,\n",
       " 7125,\n",
       " 1316,\n",
       " 3451,\n",
       " 1273,\n",
       " 6630,\n",
       " 5907,\n",
       " 1341,\n",
       " 7835,\n",
       " 5325,\n",
       " 3816,\n",
       " 2355,\n",
       " 8303,\n",
       " 5674,\n",
       " 8068,\n",
       " 7152,\n",
       " 3352,\n",
       " 4850,\n",
       " 3330,\n",
       " 6029,\n",
       " 1226,\n",
       " 5383,\n",
       " 670,\n",
       " 3557,\n",
       " 7432,\n",
       " 1212,\n",
       " 1407,\n",
       " 7910,\n",
       " 5078,\n",
       " 8009,\n",
       " 7505,\n",
       " 2953,\n",
       " 464,\n",
       " 7239,\n",
       " 4507,\n",
       " 841,\n",
       " 1663,\n",
       " 4670,\n",
       " 3761,\n",
       " 599,\n",
       " 7936,\n",
       " 4732,\n",
       " 5455,\n",
       " 4261,\n",
       " 7423,\n",
       " 1789,\n",
       " 4806,\n",
       " 565,\n",
       " 2726,\n",
       " 7359,\n",
       " 6044,\n",
       " 8262,\n",
       " 4746,\n",
       " 7303,\n",
       " 4855,\n",
       " 2913,\n",
       " 2671,\n",
       " 3459,\n",
       " 1768,\n",
       " 4080,\n",
       " 1517,\n",
       " 8343,\n",
       " 5763,\n",
       " 6752,\n",
       " 3580,\n",
       " 4065,\n",
       " 3031,\n",
       " 1604,\n",
       " 7742,\n",
       " 2074,\n",
       " 2995,\n",
       " 3234,\n",
       " 7631,\n",
       " 5161,\n",
       " 7437,\n",
       " 7287,\n",
       " 5922,\n",
       " 1943,\n",
       " 2253,\n",
       " 2266,\n",
       " 7210,\n",
       " 2540,\n",
       " 7577,\n",
       " 3072,\n",
       " 5691,\n",
       " 3255,\n",
       " 160,\n",
       " 7482,\n",
       " 7059,\n",
       " 4031,\n",
       " 259,\n",
       " 542,\n",
       " 6328,\n",
       " 4982,\n",
       " 3853,\n",
       " 1533,\n",
       " 4315,\n",
       " 2930,\n",
       " 7067,\n",
       " 4575,\n",
       " 8397,\n",
       " 4378,\n",
       " 2618,\n",
       " 1068,\n",
       " 510,\n",
       " 3916,\n",
       " 7805,\n",
       " 4318,\n",
       " 5154,\n",
       " 3438,\n",
       " 977,\n",
       " 8271,\n",
       " 1210,\n",
       " 4224,\n",
       " 2852,\n",
       " 3201,\n",
       " 472,\n",
       " 5702,\n",
       " 5852,\n",
       " 6503,\n",
       " 5815,\n",
       " 4218,\n",
       " 6836,\n",
       " 441,\n",
       " 3650,\n",
       " 7284,\n",
       " 5481,\n",
       " 5279,\n",
       " 1855,\n",
       " 7485,\n",
       " 1793,\n",
       " 832,\n",
       " 2205,\n",
       " 4234,\n",
       " 6502,\n",
       " 537,\n",
       " 4587,\n",
       " 2654,\n",
       " 3439,\n",
       " 7569,\n",
       " 8002,\n",
       " 4740,\n",
       " 5333,\n",
       " 4052,\n",
       " 4388,\n",
       " 7055,\n",
       " 6410,\n",
       " 6109,\n",
       " 5919,\n",
       " 4739,\n",
       " 414,\n",
       " 3340,\n",
       " 4840,\n",
       " 8368,\n",
       " 4477,\n",
       " 6192,\n",
       " 5459,\n",
       " 5632,\n",
       " 1929,\n",
       " 6731,\n",
       " 3163,\n",
       " 2857,\n",
       " 8001,\n",
       " 3995,\n",
       " 169,\n",
       " 5100,\n",
       " 7679,\n",
       " 2151,\n",
       " 7464,\n",
       " 7935,\n",
       " 5317,\n",
       " 4905,\n",
       " 2785,\n",
       " 7110,\n",
       " 2128,\n",
       " 6118,\n",
       " 4643,\n",
       " 4230,\n",
       " 830,\n",
       " 6820,\n",
       " 6184,\n",
       " 869,\n",
       " 7004,\n",
       " 4212,\n",
       " 5196,\n",
       " 6112,\n",
       " 2289,\n",
       " 6366,\n",
       " 3784,\n",
       " 6358,\n",
       " 5734,\n",
       " 3381,\n",
       " 93,\n",
       " 6081,\n",
       " 7841,\n",
       " 5803,\n",
       " 3153,\n",
       " 3432,\n",
       " 7725,\n",
       " 1949,\n",
       " 6238,\n",
       " 7828,\n",
       " 2108,\n",
       " 633,\n",
       " 8258,\n",
       " 4295,\n",
       " 3656,\n",
       " 5754,\n",
       " 3242,\n",
       " 2406,\n",
       " 3549,\n",
       " 571,\n",
       " 6734,\n",
       " 4702,\n",
       " 3285,\n",
       " 6634,\n",
       " 6430,\n",
       " 4720,\n",
       " 1732,\n",
       " 2422,\n",
       " 7315,\n",
       " 1648,\n",
       " 1627,\n",
       " 1652,\n",
       " 5257,\n",
       " 3204,\n",
       " 7262,\n",
       " 753,\n",
       " 1389,\n",
       " 7257,\n",
       " 4896,\n",
       " 7417,\n",
       " 7877,\n",
       " 344,\n",
       " 3192,\n",
       " 5379,\n",
       " 3446,\n",
       " 1542,\n",
       " 316,\n",
       " 3573,\n",
       " 6190,\n",
       " 225,\n",
       " 6602,\n",
       " 7379,\n",
       " 6274,\n",
       " 7427,\n",
       " 7009,\n",
       " 4245,\n",
       " 6642,\n",
       " 6768,\n",
       " 6662,\n",
       " 7917,\n",
       " 7836,\n",
       " 7237,\n",
       " 7419,\n",
       " 2739,\n",
       " 1790,\n",
       " 3960,\n",
       " 7990,\n",
       " 33,\n",
       " 3595,\n",
       " 1148,\n",
       " 6355,\n",
       " 4759,\n",
       " 4150,\n",
       " 8344,\n",
       " 1444,\n",
       " 827,\n",
       " 6898,\n",
       " 608,\n",
       " 7226,\n",
       " 1136,\n",
       " 1231,\n",
       " 6087,\n",
       " 4096,\n",
       " 8287,\n",
       " 5569,\n",
       " 8246,\n",
       " 7559,\n",
       " 1094,\n",
       " 2609,\n",
       " 5667,\n",
       " 3862,\n",
       " 3413,\n",
       " 6565,\n",
       " 2305,\n",
       " 1127,\n",
       " 2284,\n",
       " 6485,\n",
       " 6024,\n",
       " 6546,\n",
       " 2378,\n",
       " 6123,\n",
       " 8291,\n",
       " 4904,\n",
       " 1958,\n",
       " 1129,\n",
       " 1236,\n",
       " 6101,\n",
       " 7770,\n",
       " 213,\n",
       " 1552,\n",
       " 7980,\n",
       " 7509,\n",
       " 7997,\n",
       " 2604,\n",
       " 8256,\n",
       " 2054,\n",
       " 5604,\n",
       " 2762,\n",
       " 369,\n",
       " 3467,\n",
       " 5617,\n",
       " 8011,\n",
       " 3584,\n",
       " 6838,\n",
       " 5966,\n",
       " 6618,\n",
       " 286,\n",
       " 5884,\n",
       " 2300,\n",
       " 5854,\n",
       " 6621,\n",
       " 5251,\n",
       " 6884,\n",
       " 4231,\n",
       " 3159,\n",
       " 5677,\n",
       " 4856,\n",
       " 7039,\n",
       " 1595,\n",
       " 6108,\n",
       " 7850,\n",
       " 1480,\n",
       " 7788,\n",
       " 4131,\n",
       " 4008,\n",
       " 2312,\n",
       " 4001,\n",
       " 3456,\n",
       " 2254,\n",
       " 6169,\n",
       " 926,\n",
       " 4331,\n",
       " 8031,\n",
       " 8131,\n",
       " 5628,\n",
       " 3788,\n",
       " 4566,\n",
       " 501,\n",
       " 4976,\n",
       " 761,\n",
       " 32,\n",
       " 8174,\n",
       " 2666,\n",
       " 2402,\n",
       " 6858,\n",
       " 3959,\n",
       " 1863,\n",
       " 4262,\n",
       " 7629,\n",
       " 4837,\n",
       " 3937,\n",
       " 31,\n",
       " 5282,\n",
       " 302,\n",
       " 3931,\n",
       " 285,\n",
       " 41,\n",
       " 1208,\n",
       " 5059,\n",
       " 1166,\n",
       " 7952,\n",
       " 7560,\n",
       " 1902,\n",
       " ...]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bf46c7d-b957-4bb0-871c-91670b7a4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(l)\n",
    "data_train, data_test = torch.utils.data.random_split(\n",
    "    l, \n",
    "    [int(n*0.8), int(n*0.2)], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19a2fa47-c430-4d26-8302-bd9b9460a47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f840e45b160>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17228775-86f2-4716-825c-0dd9220f4ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_blocks): ModuleDict(\n",
       "    (conv_block1): ConvBlock(\n",
       "      (conv): SplineConv(1, 64, dim=3)\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block2): ConvBlock(\n",
       "      (conv): SplineConv(64, 128, dim=3)\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block3): ConvBlock(\n",
       "      (conv): SplineConv(128, 256, dim=3)\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block4): ConvBlock(\n",
       "      (conv): SplineConv(256, 512, dim=3)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (pool_blocks): ModuleDict(\n",
       "    (pool_block1): MaxPooling(voxel_size=20)\n",
       "    (pool_block2): MaxPooling(voxel_size=30)\n",
       "    (pool_block3): MaxPooling(voxel_size=50)\n",
       "    (pool_block4): MaxPoolingX(voxel_size=80, size=64)\n",
       "  )\n",
       "  (fc1): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imports.NVSModel import ResGNet, og_Net, Net\n",
    "\n",
    "model = Net(2).to('cuda')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04ffab75-ff15-4f71-9018-d9efeb08b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dl = DataLoader(l, 3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de87ee6-1381-4721-a0a1-8131f57898aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db7e4ad-773f-4654-ac1c-a180e78bf274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7028, -0.6836],\n",
       "        [-0.6094, -0.7846],\n",
       "        [-0.6874, -0.6989]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "584fa1ff-1383-4ebe-bc86-2cb4773ce326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "Acc=Accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10aa65f-adb8-4aa1-9d86-e11175028953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9631b37b-5715-4b69-8933-f2b787e52af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(epoch, train_loader):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 60:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "\n",
    "    if epoch == 110:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.00001\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_loader, desc='batches')):\n",
    "        with torch.autograd.detect_anomaly():\n",
    "            data = data.to('cuda')\n",
    "            #print(data.y)\n",
    "            optimizer.zero_grad()\n",
    "            end_point = model(data)\n",
    "            \n",
    "            loss = F.nll_loss(end_point, data.y)\n",
    "            pred = end_point.max(1)[1]\n",
    "            \n",
    "            acc = (pred.eq(data.y).sum().item())/len(data.y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 5 == 0:\n",
    "                print(data.y, pred, Acc(data.y.to('cpu'), pred.to('cpu')))\n",
    "                print({'epoch': epoch,'batch': i + 1,'loss': loss.item(),'acc': acc})\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62211bc5-8e08-47c5-9fbd-eb9e62146055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "090fa98d-ede3-4ae6-8470-c37cf08d1260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_blocks): ModuleDict(\n",
       "    (conv_block1): ConvBlock(\n",
       "      (conv): SplineConv(1, 64, dim=3)\n",
       "      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block2): ConvBlock(\n",
       "      (conv): SplineConv(64, 128, dim=3)\n",
       "      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block3): ConvBlock(\n",
       "      (conv): SplineConv(128, 256, dim=3)\n",
       "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv_block4): ConvBlock(\n",
       "      (conv): SplineConv(256, 512, dim=3)\n",
       "      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (pool_blocks): ModuleDict(\n",
       "    (pool_block1): MaxPooling(voxel_size=20)\n",
       "    (pool_block2): MaxPooling(voxel_size=30)\n",
       "    (pool_block3): MaxPooling(voxel_size=50)\n",
       "    (pool_block4): MaxPoolingX(voxel_size=80, size=64)\n",
       "  )\n",
       "  (fc1): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebbc7ceb-d17d-4311-afa4-2692294514b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56863ab1d611482fa4d1bd8d0fdbb111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/2800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14667/634645094.py:16: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 1, 'loss': 0.23057891428470612, 'acc': 1.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 6, 'loss': 6.120129108428955, 'acc': 0.0}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 11, 'loss': 0.28697383403778076, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 16, 'loss': 0.23637212812900543, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 21, 'loss': 6.32385778427124, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 26, 'loss': 2.0908076763153076, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 31, 'loss': 1.967313289642334, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 36, 'loss': 0.4439912736415863, 'acc': 1.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 41, 'loss': 13.301055908203125, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 46, 'loss': 0.22634758055210114, 'acc': 1.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 51, 'loss': 0.23970246315002441, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 56, 'loss': 8.166093826293945, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 61, 'loss': 5.996394634246826, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 66, 'loss': 7.781574726104736, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 71, 'loss': 1.7931571006774902, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 76, 'loss': 8.47495174407959, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 81, 'loss': 0.3129265308380127, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 86, 'loss': 0.23891127109527588, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 91, 'loss': 0.2462637573480606, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 96, 'loss': 0.2294723242521286, 'acc': 1.0}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 101, 'loss': 10.384016990661621, 'acc': 0.0}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 106, 'loss': 2.1585679054260254, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 111, 'loss': 5.761003494262695, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 116, 'loss': 1.5360908508300781, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 121, 'loss': 0.7249319553375244, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 126, 'loss': 0.2904082238674164, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 131, 'loss': 0.229628324508667, 'acc': 1.0}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 136, 'loss': 15.134831428527832, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 141, 'loss': 3.2443714141845703, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 146, 'loss': 0.236739382147789, 'acc': 1.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 151, 'loss': 8.930939674377441, 'acc': 0.0}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 156, 'loss': 11.327839851379395, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 161, 'loss': 0.23877833783626556, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 166, 'loss': 0.4252986013889313, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 171, 'loss': 4.628180027008057, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 176, 'loss': 2.0086591243743896, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 181, 'loss': 0.22844445705413818, 'acc': 1.0}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 186, 'loss': 5.690555095672607, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 191, 'loss': 0.26995888352394104, 'acc': 1.0}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 196, 'loss': 3.409515380859375, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 201, 'loss': 2.782094717025757, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 206, 'loss': 0.29171472787857056, 'acc': 1.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 211, 'loss': 13.662554740905762, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 216, 'loss': 7.698915004730225, 'acc': 0.0}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 221, 'loss': 2.6295113563537598, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 226, 'loss': 0.24632489681243896, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 231, 'loss': 5.597160816192627, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 236, 'loss': 0.24189631640911102, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 241, 'loss': 1.209152102470398, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 246, 'loss': 0.24554729461669922, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 251, 'loss': 1.6635710000991821, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 256, 'loss': 3.5643904209136963, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 261, 'loss': 8.888278007507324, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 266, 'loss': 14.091331481933594, 'acc': 0.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 271, 'loss': 3.4682319164276123, 'acc': 0.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 276, 'loss': 0.22872096300125122, 'acc': 1.0}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 281, 'loss': 0.2315216213464737, 'acc': 1.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 286, 'loss': 2.9957592487335205, 'acc': 0.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 291, 'loss': 2.7738704681396484, 'acc': 0.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 296, 'loss': 22.53075408935547, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 301, 'loss': 6.618730068206787, 'acc': 0.0}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 306, 'loss': 2.017194986343384, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 311, 'loss': 3.9365196228027344, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 316, 'loss': 8.415650367736816, 'acc': 0.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 321, 'loss': 3.0357859134674072, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 326, 'loss': 0.8833906650543213, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 331, 'loss': 3.4941813945770264, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 336, 'loss': 3.5702555179595947, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 341, 'loss': 10.961735725402832, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 346, 'loss': 4.66503381729126, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 351, 'loss': 10.27104663848877, 'acc': 0.0}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 356, 'loss': 0.7285283207893372, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 361, 'loss': 0.21659766137599945, 'acc': 1.0}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 366, 'loss': 1.8407820463180542, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 371, 'loss': 2.5489466190338135, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 376, 'loss': 3.06146240234375, 'acc': 0.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 381, 'loss': 0.39763346314430237, 'acc': 1.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 386, 'loss': 0.26487424969673157, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 391, 'loss': 0.2808590233325958, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 396, 'loss': 0.25155508518218994, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 401, 'loss': 1.3359347581863403, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 406, 'loss': 0.4250684976577759, 'acc': 1.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 411, 'loss': 5.268925666809082, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 416, 'loss': 4.7214460372924805, 'acc': 0.0}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 421, 'loss': 0.719695508480072, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 426, 'loss': 0.7193939089775085, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 431, 'loss': 8.085936546325684, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 436, 'loss': 4.08142614364624, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 441, 'loss': 7.059264659881592, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 446, 'loss': 0.21964240074157715, 'acc': 1.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 451, 'loss': 0.21817582845687866, 'acc': 1.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 456, 'loss': 1.9939602613449097, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 461, 'loss': 5.906282901763916, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 466, 'loss': 4.936925411224365, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 471, 'loss': 1.9867891073226929, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 476, 'loss': 4.006357192993164, 'acc': 0.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 481, 'loss': 0.2292100191116333, 'acc': 1.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 486, 'loss': 12.155341148376465, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 491, 'loss': 0.2586834132671356, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 496, 'loss': 0.2251339703798294, 'acc': 1.0}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 501, 'loss': 1.1770787239074707, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 506, 'loss': 1.53701913356781, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 511, 'loss': 1.9943557977676392, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 516, 'loss': 3.0715062618255615, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 521, 'loss': 5.663951396942139, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 526, 'loss': 3.7230262756347656, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 531, 'loss': 3.0470101833343506, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 536, 'loss': 2.8039114475250244, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 541, 'loss': 0.23497658967971802, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 546, 'loss': 8.48047924041748, 'acc': 0.0}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 551, 'loss': 0.23310315608978271, 'acc': 1.0}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 556, 'loss': 0.24998794496059418, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 561, 'loss': 17.67329216003418, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 566, 'loss': 9.99857234954834, 'acc': 0.0}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 571, 'loss': 1.1076818704605103, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 576, 'loss': 16.967988967895508, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 581, 'loss': 0.4354628026485443, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 586, 'loss': 0.2966324985027313, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 591, 'loss': 4.183593273162842, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 596, 'loss': 0.23026180267333984, 'acc': 1.0}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 601, 'loss': 5.985084056854248, 'acc': 0.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 606, 'loss': 0.22935359179973602, 'acc': 1.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 611, 'loss': 0.23438958823680878, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 616, 'loss': 0.3872191607952118, 'acc': 1.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 621, 'loss': 3.3735382556915283, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 626, 'loss': 2.021148443222046, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 631, 'loss': 1.5434776544570923, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 636, 'loss': 0.37642526626586914, 'acc': 1.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 641, 'loss': 0.4785989224910736, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 646, 'loss': 4.49489164352417, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 651, 'loss': 0.440983384847641, 'acc': 1.0}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 656, 'loss': 2.4201316833496094, 'acc': 0.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 661, 'loss': 0.21247802674770355, 'acc': 1.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 666, 'loss': 0.25195571780204773, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 671, 'loss': 3.1684608459472656, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 676, 'loss': 2.0673162937164307, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 681, 'loss': 1.8752919435501099, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 686, 'loss': 0.24314247071743011, 'acc': 1.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 691, 'loss': 4.690560817718506, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 696, 'loss': 0.6862068176269531, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 701, 'loss': 1.0789259672164917, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 756, 'loss': 0.26475343108177185, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 761, 'loss': 5.100126266479492, 'acc': 0.0}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 766, 'loss': 0.9683896899223328, 'acc': 0.3333333333333333}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 1], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 771, 'loss': 3.375568389892578, 'acc': 0.6666666666666666}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 776, 'loss': 1.1306262016296387, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 781, 'loss': 2.1079750061035156, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 786, 'loss': 2.201874256134033, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 791, 'loss': 1.3546099662780762, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 796, 'loss': 0.2342960387468338, 'acc': 1.0}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 801, 'loss': 2.332519292831421, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 806, 'loss': 2.538038730621338, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 811, 'loss': 1.2129770517349243, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 816, 'loss': 0.42151474952697754, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 821, 'loss': 1.4271732568740845, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 826, 'loss': 0.5445253252983093, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 831, 'loss': 0.42010751366615295, 'acc': 0.6666666666666666}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 836, 'loss': 2.0921080112457275, 'acc': 0.0}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([1, 0, 1], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 841, 'loss': 5.973124027252197, 'acc': 0.3333333333333333}\n",
      "tensor([1, 1, 1], device='cuda:0') tensor([1, 1, 1], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 846, 'loss': 0.22639702260494232, 'acc': 1.0}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 851, 'loss': 4.157660484313965, 'acc': 0.6666666666666666}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 856, 'loss': 0.2560383975505829, 'acc': 1.0}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 861, 'loss': 1.7481293678283691, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 866, 'loss': 0.3751819133758545, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 871, 'loss': 0.34838762879371643, 'acc': 1.0}\n",
      "tensor([0, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor(0.6667)\n",
      "{'epoch': 1, 'batch': 876, 'loss': 0.4886353015899658, 'acc': 0.6666666666666666}\n",
      "tensor([0, 1, 1], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 881, 'loss': 0.7098066210746765, 'acc': 0.3333333333333333}\n",
      "tensor([1, 0, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(0.3333)\n",
      "{'epoch': 1, 'batch': 886, 'loss': 2.4524524211883545, 'acc': 0.3333333333333333}\n",
      "tensor([0, 0, 1], device='cuda:0') tensor([1, 1, 0], device='cuda:0') tensor(0.)\n",
      "{'epoch': 1, 'batch': 891, 'loss': 4.498593807220459, 'acc': 0.0}\n",
      "tensor([0, 1, 0], device='cuda:0') tensor([0, 1, 0], device='cuda:0') tensor(1.)\n",
      "{'epoch': 1, 'batch': 896, 'loss': 0.25202974677085876, 'acc': 1.0}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 3000 but got size 2999 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#train_data_aug = T.Compose([T.Cartesian(cat=False), T.RandomFlip(axis=0, p=0.3), T.RandomScale([0.95,0.999]), T.RandomFlip(axis=1, p=0.2)])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m dl\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, train_loader)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#print(data.y)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m end_point \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(end_point, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     23\u001b[0m pred \u001b[38;5;241m=\u001b[39m end_point\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/NVSModel.py:189\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    187\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_blocks[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_block\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m](data)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m layer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_graph_layers:\n\u001b[0;32m--> 189\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpool_block\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_blocks[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool_block\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m](data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39mpos, data\u001b[38;5;241m.\u001b[39mbatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/papers_reproduction/sign_language/imports/NVSModel.py:57\u001b[0m, in \u001b[0;36mMaxPooling.forward\u001b[0;34m(self, x, pos, batch, edge_index, return_data_obj)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, pos, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, edge_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, return_data_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m edge_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index must not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m     cluster \u001b[38;5;241m=\u001b[39m \u001b[43mvoxel_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoxel_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     data \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mx, pos\u001b[38;5;241m=\u001b[39mpos, edge_index\u001b[38;5;241m=\u001b[39medge_index, batch\u001b[38;5;241m=\u001b[39mbatch)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m     data \u001b[38;5;241m=\u001b[39m max_pool(cluster, data\u001b[38;5;241m=\u001b[39mdata, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform)  \u001b[38;5;66;03m# transform for new edge attributes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch_geometric/nn/pool/voxel_grid.py:58\u001b[0m, in \u001b[0;36mvoxel_grid\u001b[0;34m(pos, size, batch, start, end)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(pos\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 58\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m size \u001b[38;5;241m=\u001b[39m size \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     60\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m start \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 3000 but got size 2999 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    \n",
    "    #train_data_aug = T.Compose([T.Cartesian(cat=False), T.RandomFlip(axis=0, p=0.3), T.RandomScale([0.95,0.999]), T.RandomFlip(axis=1, p=0.2)])\n",
    "    train_loader = dl\n",
    "    \n",
    "    train(epoch, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b8cd0-534f-438e-bcd3-94104c360942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('model.pkl')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eb36b-0bfd-41f2-96f5-50a133d8e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "log = pd.read_csv('log', names = ['epoch', 'batch', 'loss', 'acc'])\n",
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09ea97-c0fa-420e-8b18-e4c686efc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.plot(log['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648af2c-e460-417d-a39a-fdb363fa1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=log.index, y=log['loss'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc24918-cff3-4ca4-8090-8574a144879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.NVSModel import Net\n",
    "\n",
    "model = Net(\n",
    "    n_classes=2\n",
    "    #layer_sizes=[64, 128],\n",
    "    #voxel_sizes=[4, 6]\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6b516-8307-4315-a7dc-6a4d25c6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73a075-dbb5-4d84-8116-fe2b4f86d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "512*64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
